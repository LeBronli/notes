### 初始强化行为

#### 强化学习以及其相关元素

1. 正强化使得生物趋向于获得更多利益，负强化使得生物趋向于避免损害
2. 强化学习的最大特点就是学习过程中没有正确答案，而是通过奖励信号来学习



#### 案例：基于Gym库的智能体/环境交互

1. 导入Gym库后，可以通过make()函数来得到环境对象。
   1. 每个环境都有一个ID，是形如`"Xxxxx-vd"`的Python字符串
   2. `env = gym.make('CartPole-v0')`
2. 查看观测空间：`env.observation_space`
3. 查看动作空间：`env.action_space`
4. 离散空间一般用`gym.spaces.Discrete`类表示，连续空间一般用`gym.spaces.Box`表示
5. 初始化环境对象：`env.reset()`，返回智能体的初始观测，是`np.array`对象
6. 初始化环境后就可以使用环境了，使用环境的核心是step()方法，step()方法接受智能体的动作为参数，并返回4个参数
   1. 观测：`np.array`对象
   2. 奖励：float
   3. 本回合结束指示：`bool`类型的值。如果游戏结束了，可以通过`env.reset()`开始下一回合
   4. 其他信息：`dict`类型的值，含有一些调试信息
7. `env.step()`的参数需要取自动作空间。可以用`action = env.action_space.sample()`来从动作空间中随机选取一个动作
8. 每次调用step函数会让环境前进一步，所以它往往放在循环结构中，通过循环调用来完成整个回合
9. 可以使用`env.render()`来以图形化的方法来显示当前环境
10. 使用完环境后，使用`env.close()`来关闭环境
11. 如果绘制了图形界面窗口，关闭窗口的最佳方法是调用close方法。因为试图直接关闭窗口可能会导致内存不能释放，甚至导致死机
12. 测试智能体在Gym库中某个任务的性能时，一般使用100个回合的平均回合奖励
13. 智能体由我们自己实现，智能体的decide()方法实现了决策功能，而learn()方法实现了学习功能





### Markov决策过程

#### Markov决策过程模型

1. 

### 有模型数值迭代

### 回合更新价值迭代

### 时序差分价值迭代

### 函数近似方法

### 回合更新策略梯度方法

### 执行者/评论者方法

### 连续动作空间的确定性策略

### 综合案例：电动游戏

### 综合案例：棋盘游戏

### 综合案例：自动驾驶

